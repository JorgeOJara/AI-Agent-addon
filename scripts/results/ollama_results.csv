# target=http://192.168.102.245:11434
# model=llama3.2:3b
# host=Mac.spyderserve.local
# endpoint=chat
# stream=false
# sweep=1,2,5,10,15,20,30,40
# requests=50
# invocation=bun run scripts/ollama_loadtest.ts --url http://192.168.102.245:11434 --model llama3.2:3b --endpoint chat --stream false --sweep 1,2,5,10,15,20,30,40 --requests 50
concurrency,total,ok,fail,timeout,avg_ms,min_ms,p50_ms,p90_ms,p95_ms,p99_ms,max_ms,wall_s,rps
1,50,50,0,0,780.42,627,751,912,930,1018,1018,39.021,1.281
2,50,50,0,0,1223.90,624,1230,1411,1451,1481,1481,30.856,1.620
5,50,50,0,0,2962.74,932,3061,3180,3265,3379,3379,30.877,1.619
10,50,50,0,0,5832.91,866,6317,6540,6608,6803,6803,31.799,1.572
15,50,50,0,0,7967.40,911,9118,9635,9692,9920,9920,31.573,1.584
20,50,50,0,0,9289.02,881,9988,12491,12593,12680,12680,31.231,1.601
30,50,50,0,0,12766.51,883,14965,18145,18256,18627,18627,31.273,1.599
40,50,50,0,0,12981.60,1048,12403,22227,23461,24794,24794,30.837,1.621
