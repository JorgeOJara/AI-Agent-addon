# target=http://localhost:11434
# model=llama3.2
# host=Mac.spyderserve.local
# endpoint=chat
# stream=false
# sweep=1
# duration_ms=2000
# invocation=bun run scripts/ollama_loadtest.ts --url http://localhost:11434 --model llama3.2 --endpoint chat --stream false --sweep 1 --duration 2000
concurrency,total,ok,fail,timeout,avg_ms,min_ms,p50_ms,p90_ms,p95_ms,p99_ms,max_ms,wall_s,rps
1,39986,0,39986,0,0.00,0,0,0,0,0,0,1.994,0.000
